import os
from PIL import Image
import io
import base64
# å¾ transformers åº«ä¸­åƒ…ä¿ç•™ AutoProcessor
from transformers import AutoProcessor 
from llama_cpp import Llama 

# --- Base64 è¼”åŠ©å‡½æ•¸ (æ–°å¢) ---
def pil_to_base64(image: Image.Image) -> str:
    """å°‡ PIL åœ–åƒè½‰æ›ç‚º Base64 ç·¨ç¢¼çš„å­—ç¬¦ä¸² (JPEG æ ¼å¼)ã€‚"""
    # é€™è£¡é€šå¸¸ä½¿ç”¨ JPEG æ ¼å¼é€²è¡Œå£“ç¸®å’Œç·¨ç¢¼
    buffered = io.BytesIO()
    image.save(buffered, format="JPEG")
    return base64.b64encode(buffered.getvalue()).decode("utf-8")
# ------------------------------

# --- é…ç½® ---

PROCESSOR_ID = "Qwen/Qwen2.5-VL-3B-Instruct" 
GGUF_MODEL_REPO_ID = "mradermacher/VLM-R1-Qwen2.5VL-3B-OVD-0321-i1-GGUF"
# è«‹ä½¿ç”¨æ­£ç¢ºçš„æª”åï¼Œæˆ‘å€‘ä½¿ç”¨ Q4_K_M
GGUF_MODEL_FILENAME = "VLM-R1-Qwen2.5VL-3B-OVD-0321.i1-Q4_K_M.gguf" 

# 1. è¼‰å…¥ GGUF æ¨¡å‹ (ä½¿ç”¨ Llama é¡)
print(f"æ­£åœ¨è¼‰å…¥ GGUF æ¨¡å‹: {GGUF_MODEL_REPO_ID}...")
try:
    llm = Llama.from_pretrained(
        repo_id=GGUF_MODEL_REPO_ID,
        filename=GGUF_MODEL_FILENAME,
        n_ctx=32768,          
        n_gpu_layers=-1    # åœ¨ CPU ä¸Šé‹è¡Œ
    )
    print("æ¨¡å‹è¼‰å…¥æˆåŠŸã€‚")
except Exception as e:
    print(f"\nâŒ Llama è¼‰å…¥ GGUF æ¨¡å‹æ™‚ç™¼ç”ŸéŒ¯èª¤ã€‚è«‹æª¢æŸ¥é…ç½®ã€‚éŒ¯èª¤: {e}")
    exit()

# 2. è¼‰å…¥è™•ç†å™¨ (ç”¨æ–¼ç²å– Tokenizer å’Œæ¨¡å‹é…ç½®)
print(f"æ­£åœ¨è¼‰å…¥è™•ç†å™¨: {PROCESSOR_ID}...")
# é›–ç„¶æˆ‘å€‘ä¸å†ç”¨å®ƒä¾†åš Base64 è½‰æ›ï¼Œä½†ä»éœ€è¦å®ƒä¾†ç¢ºèª token è³‡è¨Š
processor = AutoProcessor.from_pretrained(PROCESSOR_ID, trust_remote_code=True)
print("è™•ç†å™¨è¼‰å…¥æˆåŠŸã€‚")

# --- æº–å‚™è¼¸å…¥ ---

IMAGE_PATH = "./test.png"  
PROMPT = "åœ–ç‰‡ä¸­äººç‰©çš„ä½ç½®åœ¨å“ªè£¡ï¼Ÿ"

if not os.path.exists(IMAGE_PATH):
    print(f"\néŒ¯èª¤ï¼šæ‰¾ä¸åˆ°åœ–ç‰‡æª”æ¡ˆ '{IMAGE_PATH}'ã€‚")
    exit()

# 3. è¼‰å…¥åœ–åƒ
image = Image.open(IMAGE_PATH).convert("RGB")
print(f"æˆåŠŸè¼‰å…¥åœ–ç‰‡: {IMAGE_PATH}")

# 4. æº–å‚™ Qwen-VL ç‰¹å®šçš„è¼¸å…¥æ ¼å¼
# ğŸ’¡ é—œéµä¿®æ­£ï¼šä½¿ç”¨æˆ‘å€‘å®šç¾©çš„ pil_to_base64 å‡½æ•¸ä¾†å–ä»£ processor.image_to_base64
encoded_image = pil_to_base64(image) # <-- ä¿®æ­£å¾Œçš„ç¨‹å¼ç¢¼

# Qwen-VL-GGUF çš„ Prompt æ ¼å¼ï¼š
full_prompt = (
    f"ç³»çµ±: ä½ æ˜¯ä¸€å€‹æ¨‚æ–¼åŠ©äººçš„è¦–è¦ºèªè¨€æ¨¡å‹ã€‚<|im_end|>\n"
    f"<|im_start|>ç”¨æˆ¶:\nåœ–ç‰‡ï¼š{encoded_image} å•é¡Œï¼š{PROMPT}<|im_end|>\n"
    f"<|im_start|>åŠ©ç†:"
)

# --- æ¨¡å‹ç”Ÿæˆ ---

print("\n--- é–‹å§‹ç”Ÿæˆå›ç­” ---")

# 5. ä½¿ç”¨ Llama é€²è¡Œæ¨ç†
output = llm(
    prompt=full_prompt,
    max_tokens=512,
    stop=["<|im_end|>"],  
    echo=False,          
    temperature=0.1
)

# 6. è¼¸å‡ºçµæœ
response_text = output["choices"][0]["text"].strip()

# æ¸…ç†å¯èƒ½çš„ç‰¹æ®ŠçµæŸæ¨™è¨˜
final_answer = response_text.replace("<|im_end|>", "").strip()

print("\nâœ… **æ¨¡å‹è¼¸å‡ºçµæœ:**")
print("---------------------------------------")
print(final_answer)
print("---------------------------------------")
